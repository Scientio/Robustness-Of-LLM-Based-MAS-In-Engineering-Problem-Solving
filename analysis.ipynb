{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c077726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import fisher_exact, mannwhitneyu, probplot\n",
    "from typing import Callable, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9606c3",
   "metadata": {},
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445005e",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(chapter: str, group: str, study: str = None) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads the base dataset and all study datasets found in subfolders of {chapter}/{group}/.\n",
    "\n",
    "    Parameters:\n",
    "        chapter (str): Top-level directory (e.g., '31_agents').\n",
    "        group (str): Sub-directory containing study folders.\n",
    "        study (str, optional): If provided, filters to only that subfolder name.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: List of loaded and preprocessed DataFrames.\n",
    "                            First is always the base dataset.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    labels_list = ['Baseline']\n",
    "\n",
    "    # Load base dataset\n",
    "    base_path = 'sensitivity/counting_base.csv'\n",
    "    if not os.path.exists(base_path):\n",
    "        raise FileNotFoundError(f\"Baseline dataset not found at {base_path}\")\n",
    "    df_base = pd.read_csv(base_path)\n",
    "    df_base['decision_reached'] = df_base['decision_reached'].astype(bool)\n",
    "    df_list.append(df_base)\n",
    "\n",
    "    # Load all study datasets from subfolders\n",
    "    parent_path = os.path.join(chapter, group)\n",
    "    if not os.path.isdir(parent_path):\n",
    "        print(\"Warning: Only baseline dataset was loaded. No study data found.\")\n",
    "        return df_list, labels_list\n",
    "\n",
    "    for folder in sorted(os.listdir(parent_path)):\n",
    "        folder_path = os.path.join(parent_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            if study and folder != study:\n",
    "                continue\n",
    "            file_path = os.path.join(folder_path, 'counting.csv')\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                if 'decision_reached' in df.columns:\n",
    "                    df['decision_reached'] = df['decision_reached'].astype(bool)\n",
    "                df_list.append(df)\n",
    "                labels_list.append(folder_path.rsplit('\\\\', 1)[-1])\n",
    "            else:\n",
    "                print(f\"Warning: counting.csv not found in {folder_path}\")\n",
    "\n",
    "    return df_list, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f0ee9",
   "metadata": {},
   "source": [
    "## Section 2: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_outcome(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classify rows into outcome categories.\n",
    "    \"\"\"\n",
    "    def classify(row):\n",
    "        val = row.get('misled')\n",
    "\n",
    "        if pd.isna(val):\n",
    "            return 'no decision'\n",
    "        # At this point, val is either True or False\n",
    "        if bool(val):  # True → 'misled'; False → 'rejected'\n",
    "            return 'misled'\n",
    "        else:\n",
    "            return 'rejected'\n",
    "        \n",
    "    df['outcome'] = df.apply(classify, axis=1)\n",
    "    return df\n",
    "\n",
    "def prepare_data(df_list: list[pd.DataFrame], study_labels: list[str]) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Classify outcomes for each DataFrame and return a dictionary mapping labels to DataFrames.\n",
    "    \"\"\"\n",
    "    if len(df_list) != len(study_labels):\n",
    "        raise ValueError(\"`df_list` and `study_labels` must have the same length.\")\n",
    "    \n",
    "    return {\n",
    "        label: classify_outcome(df.copy())\n",
    "        for label, df in zip(study_labels, df_list)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchange_items_from_csv(input_list):\n",
    "    \"\"\"\n",
    "    Replace items in input_list based on mappings defined in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_list (list of str): The list to process.\n",
    "        csv_filename (str): The CSV file name in the same directory, containing 'original' and 'replacement' columns.\n",
    "\n",
    "    Returns:\n",
    "        list of str: The modified list with items replaced based on CSV mappings.\n",
    "    \"\"\"\n",
    "    # Read the mapping from CSV into a dictionary\n",
    "    mapping = {}\n",
    "    with open('experiment_labels.csv', mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            original = row['original'].strip()\n",
    "            replacement = row['replacement'].strip()\n",
    "            mapping[original] = replacement\n",
    "\n",
    "    # Replace items in the list if they exist in the mapping\n",
    "    replaced_list = [mapping.get(item, item) for item in input_list]\n",
    "\n",
    "    return replaced_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391888dc",
   "metadata": {},
   "source": [
    "## Section 3: Outcome Distribution Analysis Across Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcome_distribution_across_trials(\n",
    "    df_dict: dict[str, pd.DataFrame],\n",
    "    trial_steps: list[int]\n",
    ") -> pd.DataFrame:\n",
    "    results = []\n",
    "    for study, df in df_dict.items():\n",
    "        for n in trial_steps:\n",
    "            if n > len(df):\n",
    "                continue\n",
    "            subset = df.iloc[:n]\n",
    "            counts = subset['outcome'].value_counts(normalize=True) * 100\n",
    "            results.append({\n",
    "                'study': study,\n",
    "                'Number of Trials': n,\n",
    "                'misled': counts.get('misled', 0),\n",
    "                'rejected': counts.get('rejected', 0),\n",
    "                'no decision': counts.get('no decision', 0)\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def plot_outcome_distribution(plot_df: pd.DataFrame, study: str):\n",
    "    colors = ['#009688', '#B22222', '#A9A9A9']\n",
    "    study_df = plot_df[plot_df['study'] == study].set_index('Number of Trials')\n",
    "    ax = study_df[['misled', 'rejected', 'no decision']].plot(\n",
    "        kind='bar', stacked=True, color=colors, figsize=(10,6)\n",
    "    )\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.0f%%', label_type='center', fontsize=9, color='black')\n",
    "    ax.set_title(f'Outcome Distribution by Number of Trials ({study})')\n",
    "    ax.set_xlabel('Number of Trials')\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xticklabels(study_df.index, rotation=0)\n",
    "    ax.set_yticklabels([f'{int(t)}%' for t in ax.get_yticks()])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ddfcd",
   "metadata": {},
   "source": [
    "## Section 4: Distribution Stability Over Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e9875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(data: list[str], categories: list[str]) -> np.ndarray:\n",
    "    counts = Counter(data)\n",
    "    return np.array([counts.get(cat, 0) for cat in categories], dtype=float) / len(data)\n",
    "\n",
    "def total_variation_distance(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    return 0.5 * np.sum(np.abs(p - q))\n",
    "\n",
    "def plot_distribution_stability(outcomes: list[str], categories: list[str], study: str, step=1):\n",
    "    tvd_vals, tvd_vals_next, jsd_vals, trial_sizes = [], [], [], []\n",
    "    final_dist = get_distribution(outcomes, categories)\n",
    "\n",
    "    for n in range(step, len(outcomes) - step, step):\n",
    "        dist_n = get_distribution(outcomes[:n], categories)\n",
    "        dist_n_next = get_distribution(outcomes[:n + step], categories)\n",
    "        tvd_vals_next.append(total_variation_distance(dist_n, dist_n_next))\n",
    "        tvd_vals.append(total_variation_distance(dist_n, final_dist))\n",
    "        jsd_vals.append(jensenshannon(dist_n, dist_n_next, base=2))\n",
    "        trial_sizes.append(n)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(trial_sizes, tvd_vals, label='TVD vs Final', marker='o')\n",
    "    plt.plot(trial_sizes, tvd_vals_next, label='TVD vs Next', marker='x')\n",
    "    plt.axhline(0.05, color='gray', linestyle='--', label='Threshold (0.05)')\n",
    "    plt.xlabel('Number of Trials')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.title(f'Distribution Stability ({study})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c2883",
   "metadata": {},
   "source": [
    "## Section 5: Comparing Multiple Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a62da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for LaTeX fonts to match document\n",
    "fntsz = 15\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern\"],\n",
    "    \"font.size\": fntsz,\n",
    "    \"axes.titlesize\": fntsz,\n",
    "    \"axes.labelsize\": fntsz,\n",
    "    \"xtick.labelsize\": fntsz,\n",
    "    \"ytick.labelsize\": fntsz,\n",
    "    \"legend.fontsize\": fntsz,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(df_list: list, title: str):\n",
    "    \"\"\"\n",
    "    Plot comparative bar charts of outcome proportions for multiple DataFrames.\n",
    "    \"\"\"\n",
    "    ref_categories = sorted(set(df_list[0]['outcome'].unique()))\n",
    "    n_dfs = len(df_list)\n",
    "    bar_width = 0.2\n",
    "    x = np.arange(len(ref_categories))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        counts = df['outcome'].value_counts(normalize=True).reindex(ref_categories, fill_value=0)\n",
    "        label = 'Baseline Distribution' if i == 0 else f'DataFrame {i+1}'\n",
    "        bars = ax.bar(x + (i - (n_dfs-1)/2)*bar_width, counts, width=bar_width, label=label, alpha=0.7)\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, height,\n",
    "                    f'{height*100:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(ref_categories)\n",
    "    ax.set_xlabel('Outcome')\n",
    "    ax.set_ylabel('Proportion')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_jsd_tvd(df_dict: dict[str, pd.DataFrame], print_output: bool = True):\n",
    "    \"\"\"\n",
    "    Calculate and print Jensen-Shannon Divergence and Total Variation Distance\n",
    "    for each DataFrame in the dict compared to the base DataFrame.\n",
    "    \"\"\"\n",
    "    base_df = df_dict['Baseline']\n",
    "    categories = sorted(base_df['outcome'].dropna().unique())\n",
    "\n",
    "    results = []\n",
    "    base_dist = base_df['outcome'].value_counts(normalize=True).reindex(categories, fill_value=0)\n",
    "\n",
    "    for experiment, df in df_dict.items():\n",
    "        if experiment == 'Baseline':\n",
    "            continue\n",
    "\n",
    "        dist = df['outcome'].value_counts(normalize=True).reindex(categories, fill_value=0)\n",
    "\n",
    "        jsd = jensenshannon(base_dist, dist, base=2)\n",
    "        tvd = np.sum(np.abs(base_dist - dist)) / 2\n",
    "\n",
    "        if print_output:\n",
    "            print(f\"\\n--- Comparison with '{experiment}' ---\")\n",
    "            print(f\"JSD: {jsd:.4f}\")\n",
    "            print(f\"TVD: {tvd:.4f}\")\n",
    "        results.append({\n",
    "            'experiment': experiment,\n",
    "            'jsd': jsd,\n",
    "            'tvd': tvd\n",
    "        })\n",
    "    return pd.DataFrame(results).set_index('experiment')\n",
    "\n",
    "def compute_significance_band(df: pd.DataFrame, alpha: float, resilience: bool = True) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute the significance band of acceptable proportions for a second sample\n",
    "    based on Fisher's Exact Test comparing to the input dataframe's outcome.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe with a binary 'outcome' column.\n",
    "        resilience (bool): If False, test against 'misled'; otherwise, 'rejected'.\n",
    "        alpha (float): Significance level\n",
    "        \n",
    "    Returns:\n",
    "        (min_prop, max_prop): Tuple of minimum and maximum proportion where p >= alpha.\n",
    "    \"\"\"\n",
    "    target = 'rejected' if resilience else 'misled'\n",
    "    \n",
    "    # Actual counts\n",
    "    outcome_counts = df['outcome'].value_counts()\n",
    "    true_count = outcome_counts.get(target, 0)\n",
    "    false_count = df['outcome'].notna().sum() - true_count\n",
    "    n = true_count + false_count\n",
    "    \n",
    "    # Scan possible outcomes in second sample (same size n)\n",
    "    p_values = []\n",
    "    for test_true in range(0, n + 1):\n",
    "        test_false = n - test_true\n",
    "        \n",
    "        # Contingency table: [ [base_true, base_false], [test_true, test_false] ]\n",
    "        table = [[true_count, false_count], [test_true, test_false]]\n",
    "        _, p = fisher_exact(table, alternative='two-sided')\n",
    "        p_values.append((test_true / n, p))\n",
    "    \n",
    "    # Filter for p >= alpha (non-significant region)\n",
    "    non_sig = [prop for prop, p in p_values if p >= alpha]\n",
    "    \n",
    "    if not non_sig:\n",
    "        return (np.nan, np.nan)  # No band found (very extreme base case)\n",
    "    \n",
    "    return (min(non_sig), max(non_sig))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc6a5d",
   "metadata": {},
   "source": [
    "## Section 6: Statistical Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_binary_counts(\n",
    "    df_base: pd.DataFrame,\n",
    "    df_comp: pd.DataFrame,\n",
    "    column: str,\n",
    "    true_condition: Callable[[pd.Series], pd.Series]\n",
    ") -> Dict[str, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Calculate binary outcome counts for base and comparison DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        Dict with keys 'base' and 'comp', and values as (count_true, count_false).\n",
    "    \"\"\"\n",
    "    def count_true_false(df: pd.DataFrame) -> Tuple[int, int]:\n",
    "        col_data = df[column]\n",
    "        mask = true_condition(col_data)\n",
    "        return mask.sum(), (~mask).sum()\n",
    "\n",
    "    return {\n",
    "        \"base\": count_true_false(df_base),\n",
    "        \"comp\": count_true_false(df_comp)\n",
    "    }\n",
    "\n",
    "def run_fisher_test(df_base: pd.DataFrame, df_comp: pd.DataFrame, column: str, condition_func, label: str, experiment: str, print_output: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Run Fisher's Exact Test for binary categorical column across multiple dataframes.\n",
    "    \"\"\"\n",
    "    results = calculate_binary_counts(df_base, df_comp, column, condition_func)\n",
    "    \n",
    "    base_counts = list(results[\"base\"])\n",
    "    comp_counts = list(results[\"comp\"])\n",
    "\n",
    "    table = [base_counts, comp_counts]\n",
    "    \n",
    "    oddsratio, pval = fisher_exact(table)\n",
    "    \n",
    "    if print_output:\n",
    "        print(f\"\\n--- {label} (Fisher's Exact Test) ---\")\n",
    "        print(pd.DataFrame(table, columns=['True', 'False'], index=[f'Baseline', experiment]))\n",
    "        print(f\"Odds Ratio: {oddsratio:.4f}\")\n",
    "        print(f\"p-value: {pval:.4e}\")\n",
    "        print(\"Result:\", \"Significant difference.\" if pval < 0.05 else \"No significant difference.\")\n",
    "\n",
    "        if pval < 0.05:\n",
    "            print(f\"Significant difference in {label} distribution detected.\")\n",
    "            # plot boxplots base vs experiment next to each other\n",
    "            base_correct = df_base[label].dropna()\n",
    "            sample_correct = df_comp[label].dropna()\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.boxplot([base_correct, sample_correct], labels=['Baseline', experiment])\n",
    "            plt.title(f'{label} Comparison - {experiment}')\n",
    "            plt.ylabel(label)\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    return {\n",
    "        'label': label,\n",
    "        'test': 'Fisher\\'s Exact Test',\n",
    "        'p-value': pval,\n",
    "        'significant': True if pval < 0.05 else False\n",
    "    }\n",
    "\n",
    "def run_mannwhitney_test(df_base: pd.DataFrame, df_comp: pd.DataFrame, column: str, label: str, experiment: str, print_output: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Run Mann-Whitney U test to compare distribution of a continuous/numerical variable.\n",
    "    \"\"\"\n",
    "    df_list = [df_base, df_comp]\n",
    "    base = df_list[0][column].dropna()\n",
    "    \n",
    "    sample = df_comp[column].dropna()\n",
    "    stat, pval = mannwhitneyu(sample, base, alternative='two-sided')\n",
    "    \n",
    "    if print_output:\n",
    "        # Q-Q Plot (optional, still useful for visualization)\n",
    "        probplot(sample, dist=\"norm\", plot=plt)\n",
    "        plt.title(f\"Q-Q Plot - {experiment}\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\n--- {label} (Mann–Whitney U Test) ---\")\n",
    "        print(f\"Mean {experiment}: {sample.mean():.2f}, Mean Base: {base.mean():.2f}\")\n",
    "        print(f\"Median {experiment}: {sample.median():.2f}, Median Base: {base.median():.2f}\")\n",
    "        print(f\"U statistic: {stat:.2f}, p-value: {pval:.4e}\")\n",
    "        print(\"Result:\", \"Significant difference.\" if pval < 0.05 else \"No significant difference.\")\n",
    "\n",
    "    return {\n",
    "            'label': label,\n",
    "            'test': 'Mann-Whitney U Test',\n",
    "            'p-value': pval,\n",
    "            'significant': True if pval < 0.05 else False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9dc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correctness_ratio(df_list: list, experiment: str) -> dict:\n",
    "    ratios = {}\n",
    "    for i, df in enumerate(df_list):\n",
    "        not_misled = df['misled'] == False if 'misled' in df.columns else ~df['outcome'].eq('misled')\n",
    "        correct_false = ((not_misled) & (df['correct'] == False)).sum()\n",
    "        correct_true = ((not_misled) & (df['correct'] == True)).sum()\n",
    "        ratio = correct_false / correct_true if correct_true != 0 else np.nan\n",
    "        ratios[experiment] = (ratio, [correct_false, correct_true])\n",
    "    return ratios\n",
    "\n",
    "def calculate_correctness_table(df_list: list) -> list:\n",
    "    table = []\n",
    "    for df in df_list:\n",
    "        not_misled = df['misled'] == False\n",
    "        correct_false = ((not_misled) & (df['correct'] == False)).sum()\n",
    "        remaining = len(df) - correct_false\n",
    "        table.append((correct_false, remaining))\n",
    "    return table\n",
    "\n",
    "\n",
    "def compare_correctness_ratios(base_df: pd.DataFrame, comp_df: pd.DataFrame, experiment: str, print_output: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Compare correctness ratio across multiple datasets using Fisher's Exact Test.\n",
    "    Assumes binary correctness (True/False).\n",
    "    \"\"\"\n",
    "    if 'correct' not in comp_df.columns:\n",
    "        print(f\"The 'correct' column is missing in the DataFrame of experiment {experiment}. Test for correctness skipped.\")\n",
    "        return {\n",
    "            'label': 'Correctness',\n",
    "            'test': 'Fisher\\'s Exact Test',\n",
    "            'p-value': None,\n",
    "            'significant': None\n",
    "        }\n",
    "    df_list = [base_df, comp_df]\n",
    "    table = calculate_correctness_table(df_list)\n",
    "    oddsratio, pval = fisher_exact(table)\n",
    "    \n",
    "    \n",
    "    if print_output:\n",
    "        print(f\"\\n--- Correctness Ratio Comparison ({experiment}) ---\")\n",
    "        labels = ['Baseline', experiment]\n",
    "        for label, (incorrect, remaining) in zip(labels, table):\n",
    "            ratio = incorrect / remaining if remaining != 0 else float('nan')\n",
    "            print(f\"{label} Correctness Ratio: {ratio:.2f} ({incorrect}/{remaining})\")\n",
    "        print(\"\\nContingency Table (Incorrect / Remaining):\")\n",
    "        print(pd.DataFrame(table, columns=[\"Incorrect\", \"Remaining\"], index=labels))\n",
    "        print(f\"\\nFisher's Exact Test p-value: {pval:.4e}\")\n",
    "        print(\"Result:\", \"Significant difference.\" if pval < 0.05 else \"No significant difference.\")\n",
    "\n",
    "    return {\n",
    "            'label': 'Correctness',\n",
    "            'test': 'Fisher\\'s Exact Test',\n",
    "            'p-value': pval,\n",
    "            'significant': True if pval < 0.05 else False\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c06a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stat_tests(df_dict: dict[str, pd.DataFrame], resilience: bool, print_output: bool) -> pd.DataFrame:\n",
    "    if len(df_dict) < 2:\n",
    "        raise ValueError(\"At least two dataframes are required to compare.\")\n",
    "    \n",
    "    target = 'rejected' if resilience else 'misled'\n",
    "    \n",
    "    results = []\n",
    "    base_df = df_dict['Baseline']\n",
    "\n",
    "    for experiment, comp_df in {k: v for k, v in df_dict.items() if k != 'Baseline'}.items():\n",
    "        required_cols = {'misled', 'decision_reached', 'iterations_needed'}\n",
    "        if not required_cols.issubset(comp_df.columns):\n",
    "            print(f\"Skipping {experiment}: required columns not found in base DataFrame.\")\n",
    "            continue\n",
    "        print(f\"\\n--- Running Statistical Tests for Base Case vs. '{experiment}' ---\")\n",
    "\n",
    "        # Append with experiment key\n",
    "        results.append({**run_fisher_test(base_df, comp_df, 'misled', lambda col: col == (not resilience), label = target, experiment=experiment, print_output=print_output), 'experiment': experiment})\n",
    "        results.append({**run_fisher_test(base_df, comp_df, 'decision_reached', lambda col: col == True, \"decision_reached\", experiment=experiment, print_output=print_output), 'experiment': experiment})\n",
    "        results.append({**run_mannwhitney_test(base_df, comp_df, 'iterations_needed', \"iterations_needed\", experiment, print_output=print_output), 'experiment': experiment})\n",
    "        results.append({**compare_correctness_ratios(base_df, comp_df, experiment, print_output=print_output), 'experiment': experiment})\n",
    "\n",
    "    return pd.DataFrame(results).set_index('experiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95f41b",
   "metadata": {},
   "source": [
    "## Section 7: Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7fe4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_comparison(df_dict: dict[str, pd.DataFrame], resilience: bool):\n",
    "    \"\"\"\n",
    "    Creates a single stacked bar chart showing outcome proportions for each study and a second set of bars\n",
    "    for iterations_needed next to the outcome bars.\n",
    "\n",
    "    Parameters:\n",
    "        df_dict (dict[str, pd.DataFrame]): Dict with dataframes and study labels\n",
    "        resilience (bool): Whether to use resilience-based sorting or not.\n",
    "        title (str): Plot title.\n",
    "    \"\"\"\n",
    "    study_labels = list(df_dict.keys())\n",
    "    df_list = list(df_dict.values())\n",
    "    \n",
    "    assert len(df_list) == len(study_labels), \"study_labels must match df_list in length\"\n",
    "\n",
    "    outcomes = ['misled', 'no decision', 'rejected']\n",
    "    colors = {\n",
    "        'misled': \"#B33131\",\n",
    "        'no decision': \"#707070\",\n",
    "        'rejected': \"#2CAA8F\",\n",
    "        'non_sig_min': \"#9B7C4F\",  # Color for non-significant minimum\n",
    "        'non_sig_max': \"#F1A83B\"   # Color for non-significant maximum\n",
    "    }\n",
    "\n",
    "    # Compute misled proportions and sort accordingly\n",
    "    misled_props = [df['outcome'].value_counts(normalize=True).get('misled', 0) for df in df_list]\n",
    "    misled_false_props = [(df['misled'] == False).mean() if 'misled' in df.columns else np.nan for df in df_list]\n",
    "    sorted_indices = np.argsort(misled_false_props) if resilience else np.argsort(misled_props)\n",
    "\n",
    "    # Compute significance band\n",
    "    alf = 0.05\n",
    "    non_sig_min, non_sig_max = compute_significance_band(df_list[0], alpha=alf, resilience=resilience)\n",
    "    \n",
    "    # Reorder both df_list and study_labels\n",
    "    df_list = [df_list[i] for i in sorted_indices]\n",
    "    study_labels = [study_labels[i] for i in sorted_indices]\n",
    "\n",
    "    # Prepare data: get proportions for each outcome in each study\n",
    "    proportions = {outcome: [] for outcome in outcomes}\n",
    "    iterations_needed = []\n",
    "    iterations_std = []\n",
    "\n",
    "    for df in df_list:\n",
    "        counts = df['outcome'].value_counts(normalize=True)\n",
    "        for outcome in outcomes:\n",
    "            proportions[outcome].append(counts.get(outcome, 0))\n",
    "        iterations_needed.append(df['iterations_needed'].mean())\n",
    "        iterations_std.append(df['iterations_needed'].std())\n",
    "  \n",
    "    n_groups = len(df_list)\n",
    "    fixed_bar_width = 0.4\n",
    "    group_spacing = fixed_bar_width * 2  # Space between groups of bars\n",
    "    x = np.arange(n_groups) * group_spacing\n",
    "    padding = fixed_bar_width / 2  # Padding for sides\n",
    "\n",
    "     # Adjust figure width dynamically, e.g. base width of 4 + extra per group\n",
    "    base_width = 4\n",
    "    width_per_group = 0.9  # inches per group (tweak as needed)\n",
    "    fig_width = min(base_width + n_groups * width_per_group, 14)  # max width of 14 inches\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(fig_width, 6))\n",
    "\n",
    "    bottom = np.zeros(len(df_list))\n",
    "    for outcome in outcomes:\n",
    "        vals = proportions[outcome]\n",
    "        bar_colors = [colors[outcome]] * len(vals)\n",
    "        bars = ax1.bar(x - fixed_bar_width / 2, vals, bottom=bottom, width=fixed_bar_width, label=outcome,\n",
    "                       color=bar_colors, edgecolor='white')\n",
    "\n",
    "        # Add percentage labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            if height > 0.04:  # Only label if the section is big enough\n",
    "                ax1.text(bar.get_x() + bar.get_width() / 2,\n",
    "                         bottom[i] + height / 2,\n",
    "                         f'{height * 100:.0f}\\\\%',\n",
    "                         ha='center', va='center', color='black', fontweight='bold')\n",
    "        bottom += vals\n",
    "\n",
    "    # Set the labels and title for the primary axis\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(study_labels, rotation=45, ha='right')\n",
    "    ax1.set_xlim(x[0] - fixed_bar_width - padding, x[-1] + fixed_bar_width / 2 + padding)\n",
    "    ax1.set_ylabel('Proportion of Outcomes')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.yaxis.set_major_formatter(PercentFormatter(1.0))  # Format y-axis as percent based on fraction (1.0 = 100%)\n",
    "    # ax1.set_title(title, fontsize=14, weight='bold')\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Plot horizontal lines for non_sig_min and non_sig_max (Lower Significance Threshold and Upper Significance Threshold)\n",
    "    non_sig_max_plot = 1 - non_sig_max  if resilience else non_sig_max  # Adjust for resilience case\n",
    "    non_sig_min_plot = 1 - non_sig_min if resilience else non_sig_min  # Adjust for resilience case\n",
    "    ax1.axhline(non_sig_max_plot, color=colors['non_sig_max'], linestyle='--', label=f'Upper Confidence Bound ({non_sig_max * 100:.0f}\\\\%) ($\\\\alpha = {alf}$) [{\"rejected\" if resilience else \"misled\"}]')\n",
    "    ax1.axhline(non_sig_min_plot, color=colors['non_sig_min'], linestyle='--', label=f'Lower Confidence Bound ({non_sig_min * 100:.0f}\\\\%) ($\\\\alpha = {alf}$) [{\"rejected\" if resilience else \"misled\"}]')\n",
    "\n",
    "    # Add the legend for main outcome plots\n",
    "    fig.legend(title='Outcome', loc='upper left', bbox_to_anchor=(-0.5, 0.03), ncol=3)\n",
    "\n",
    "    \n",
    "    # Create separate bars for iterations_needed\n",
    "    ax2 = ax1.twinx()  # Create a secondary y-axis for iterations\n",
    "    ax2.set_ylabel('Number of Interaction Cycles')\n",
    "    ax2.set_ylim(0, 6)  # Set y-axis limit for iterations_needed\n",
    "    bars2 = ax2.bar(x + 0.5*fixed_bar_width / 2, iterations_needed, width=fixed_bar_width/2, facecolor='none', edgecolor='black', linewidth=1, label='Iterations', yerr=iterations_std, capsize=5, error_kw=dict(ecolor='black', lw=1))\n",
    "    \n",
    "    # Add the legend for iterations\n",
    "    errorbar_proxy = Line2D([0], [0], color='black', lw=1.5, linestyle='-', label='Std Dev (error bars)')\n",
    "    fig.legend(handles=[bars2, errorbar_proxy], title='Iterations', loc='upper left', bbox_to_anchor=(-0.5, -0.15), ncol=1) #0.75, -0.18\n",
    "\n",
    "    # Display the plot\n",
    "    #plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbb90e",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "chapter = 'experiments/31_agents' #31_agents, 32_problem_setting, 33_system_design\n",
    "group = 'none' # 'number_of_advisors', interaction_moderator_iter10\n",
    "STUDY = 'decentral_smm'\n",
    "\n",
    "# Load and prepare data\n",
    "df_list, study_labels_raw = load_data(chapter, group)\n",
    "\n",
    "# Collect indices where labels contain those keywords\n",
    "keywords = ('next_step', 'find_path_assisting', 'decentral_sm_open', 'FDm_APs_PEs', 'mms_named', 'ms_named')\n",
    "indices_to_remove = [i for i, label in enumerate(study_labels_raw) \n",
    "                     if any(keyword in label for keyword in keywords)]\n",
    "\n",
    "# Remove items from df_list and study_labels_raw in reverse order\n",
    "for i in sorted(indices_to_remove, reverse=True):\n",
    "    df_list.pop(i)\n",
    "    study_labels_raw.pop(i)\n",
    "\n",
    "# add SMM and MSM for comparison\n",
    "# df_smm = pd.read_csv('33_system_design/number_of_advisors/smm/counting.csv')\n",
    "# df_msm = pd.read_csv('33_system_design/number_of_advisors/msm/counting.csv')\n",
    "# df_smm['decision_reached'] = df_smm['decision_reached'].astype(bool)\n",
    "# df_msm['decision_reached'] = df_msm['decision_reached'].astype(bool)\n",
    "# df_list.append(df_smm)\n",
    "# df_list.append(df_msm)\n",
    "# study_labels_raw.append('SMM')\n",
    "# study_labels_raw.append('MSM')\n",
    "\n",
    "# exchange study labels\n",
    "study_labels = exchange_items_from_csv(study_labels_raw)\n",
    "# for df in df_list[1:]:\n",
    "#     df.drop(['agents misled','agents rejected'], axis=1, inplace=True)\n",
    "df_dict_prepared = prepare_data(df_list, study_labels)\n",
    "\n",
    "# Display first entry of dict for sanity check\n",
    "# print(df_dict_prepared['Baseline'].head())\n",
    "\n",
    "# Analyze outcome distribution over trials\n",
    "trial_steps = [2, 3, 5, 10, 15, 20, 30]\n",
    "plot_df = get_outcome_distribution_across_trials(df_dict_prepared, trial_steps)\n",
    "#plot_outcome_distribution(plot_df, STUDY)\n",
    "\n",
    "# Plot stability of distribution of base case\n",
    "categories = ['misled', 'rejected', 'no decision']\n",
    "outcomes_list = df_dict_prepared[next(iter(df_dict_prepared))]['outcome'].tolist()\n",
    "#plot_distribution_stability(outcomes_list, categories, STUDY)\n",
    "\n",
    "# Compare multiple datasets' distributions visually\n",
    "resilience = True if 'lead' in group else False\n",
    "plot_multi_comparison(df_dict_prepared, resilience = resilience)\n",
    "\n",
    "# Calculate and print divergence metrics between base and others\n",
    "tvd_df = calculate_jsd_tvd(df_dict_prepared, print_output=False)\n",
    "\n",
    "# Run statistical tests - comparison to base case\n",
    "stats_df = run_stat_tests(df_dict_prepared, resilience = resilience, print_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68ea88",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.to_csv(f'{os.path.join(chapter, group)}/stats_results.csv', index=True)\n",
    "print(f\"The significant statistics for experiment {group}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd860b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = stats_df.dropna()\n",
    "filtered_df = filtered_df[filtered_df['significant']].drop(columns=['significant'])\n",
    "filtered_df.to_csv(f'{os.path.join(chapter, group)}/stats_results_significant.csv', index=True)\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd9953",
   "metadata": {},
   "source": [
    "# Summarize Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "def summarize_csv(file_path):\n",
    "    total_rows = 0\n",
    "    misled_count = 0\n",
    "    reject_count = 0\n",
    "    decision_count = 0\n",
    "    correct_count = 0\n",
    "    total_iterations = 0\n",
    "    valid_iteration_rows = 0\n",
    "\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            total_rows += 1\n",
    "\n",
    "            # Count iterations if present\n",
    "            try:\n",
    "                iterations = int(row['iterations_needed'])\n",
    "                total_iterations += iterations\n",
    "                valid_iteration_rows += 1\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "\n",
    "            # Check decision_reached\n",
    "            if row['decision_reached'].strip().lower() == 'true':\n",
    "                decision_count += 1\n",
    "\n",
    "            # Check misled\n",
    "            if row['misled'].strip().lower() == 'true':\n",
    "                misled_count += 1\n",
    "\n",
    "            # Check rejected (misled == false)\n",
    "            if row['misled'].strip().lower() == 'false':\n",
    "                reject_count += 1\n",
    "\n",
    "                # Check correctness only if rejected\n",
    "                if 'correct' in row and row['correct'].strip().lower() == 'true':\n",
    "                    correct_count += 1\n",
    "\n",
    "    summary = {\n",
    "        'misleading_rate': round((misled_count / total_rows) * 100, 2) if total_rows else 0,\n",
    "        'rejecting_rate': round((reject_count / total_rows) * 100, 2) if total_rows else 0,\n",
    "        'decision_reached_rate': round((decision_count / total_rows) * 100, 2) if total_rows else 0,\n",
    "        'average_iterations_needed': round(total_iterations / valid_iteration_rows, 2) if valid_iteration_rows else 0,\n",
    "        'correctness_rate': round((correct_count / reject_count) * 100, 2) if reject_count else 0\n",
    "    }\n",
    "\n",
    "    return summary\n",
    "\n",
    "# === Build Summary Dictionary ===\n",
    "summary_dict = {}\n",
    "\n",
    "# Automatically detect chapters (top-level directories)\n",
    "chapters = ['31_agents', '32_problem_setting', '33_system_design']\n",
    "for chapter in chapters:\n",
    "    if not os.path.isdir(chapter):\n",
    "        continue  # Skip files\n",
    "\n",
    "    summary_dict[chapter] = {}\n",
    "    chapter_path = os.path.join(chapter)\n",
    "\n",
    "    # Automatically detect groups within each chapter\n",
    "    for group in sorted(os.listdir(chapter_path)):\n",
    "        group_path = os.path.join(chapter_path, group)\n",
    "        if not os.path.isdir(group_path):\n",
    "            continue\n",
    "\n",
    "        summary_dict[chapter][group] = {}\n",
    "\n",
    "        # Loop through study folders inside each group\n",
    "        for folder in sorted(os.listdir(group_path)):\n",
    "            folder_path = os.path.join(group_path, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(folder_path, 'counting.csv')\n",
    "            if os.path.exists(file_path):\n",
    "                result = summarize_csv(file_path)\n",
    "                summary_dict[chapter][group][folder] = result\n",
    "            else:\n",
    "                print(f\"[Missing] {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ec441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write summary to file\n",
    "with open(\"summary_results.json\", \"w\") as f:\n",
    "    json.dump(summary_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
